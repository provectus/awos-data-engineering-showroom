# Technical Specification: Demand Forecasting Engine - Station Rebalancing Predictions

- **Functional Specification:** `context/spec/004-demand-forecasting-engine/functional-spec.md`
- **Status:** Draft
- **Author(s):** Engineering Team

---

## 1. High-Level Technical Approach

This feature creates a **scenario-based simulation tool** (not a traditional time-series forecast) that lets operations managers predict net bike flow patterns by selecting area, day-of-week, weather, and holiday conditions.

**Implementation Strategy:**

1. **Create station clustering** using k-means (k=30) ‚Üí store in `dim_station_clusters` dbt model
2. **Build historical baseline mart** (`mart_baseline_net_flow`) that pre-aggregates hourly net flow patterns by area + day-of-week + weather + holiday type
3. **Create interactive Streamlit dashboard** (`pages/Demand_Forecast.py`) with:
   - Map-based area selection (click cluster on map)
   - Input controls (day-of-week, weather, holiday type dropdowns)
   - 24-hour chart showing predicted net flow (absolute bikes to add/remove)
   - Station list showing all stations in selected cluster
4. **Forecasting logic** is a simple lookup: query the baseline mart filtered by user inputs

**Systems Affected:**
- `dbt/models/core/dim_station_clusters.sql` - NEW: K-means cluster assignments
- `dbt/models/marts/mart_baseline_net_flow.sql` - NEW: Historical patterns by area/day/weather/holiday/hour
- `streamlit_app/pages/Demand_Forecast.py` - NEW: Simulation dashboard
- `scripts/generate_station_clusters.py` - NEW: Python script to generate k-means clusters

**No changes to existing working code:** dlt pipelines, existing marts, existing Streamlit pages remain unchanged.

---

## 2. Proposed Solution & Implementation Plan (The "How")

### Data Model / Database Changes

#### New Model: `dim_station_clusters` (Core Layer)

**File:** `dbt/models/core/dim_station_clusters.sql`

**Purpose:** Store k-means cluster assignments for all stations (pre-computed, consistent across all queries)

**Approach:** Since dbt is SQL-only and k-means requires Python, we'll generate clusters using a **one-time Python script** that outputs a CSV seed file.

**Script:** `scripts/generate_station_clusters.py`

```python
import duckdb
import pandas as pd
from sklearn.cluster import KMeans

# Connect to warehouse
conn = duckdb.connect('duckdb/warehouse.duckdb')

# Get all stations with coordinates
stations = conn.execute("""
    SELECT station_id, station_name, latitude, longitude
    FROM main_core.dim_stations
    WHERE latitude IS NOT NULL AND longitude IS NOT NULL
""").df()

# Run k-means clustering
coords = stations[['latitude', 'longitude']].values
kmeans = KMeans(n_clusters=30, random_state=42, n_init=10)
stations['cluster_id'] = kmeans.fit_predict(coords)

# Calculate cluster centroids
cluster_centroids = pd.DataFrame(
    kmeans.cluster_centers_,
    columns=['centroid_lat', 'centroid_lon']
)
cluster_centroids['cluster_id'] = range(30)

# Merge stations with centroids
stations_with_clusters = stations.merge(
    cluster_centroids, on='cluster_id', how='left'
)

# Save to dbt seed file
stations_with_clusters.to_csv(
    'dbt/seeds/station_clusters.csv',
    index=False
)
print(f"Generated {len(stations)} station assignments across 30 clusters")
conn.close()
```

**dbt Seed:** `dbt/seeds/station_clusters.csv` (generated by script above)

**dbt Model:** `dbt/models/core/dim_station_clusters.sql`

```sql
{{
    config(
        materialized='table'
    )
}}

select
    station_id,
    station_name,
    latitude,
    longitude,
    cluster_id,
    centroid_lat,
    centroid_lon,
    -- Calculate distance from station to cluster centroid (km) using Haversine
    2 * 6371 * asin(sqrt(
        pow(sin(radians(centroid_lat - latitude) / 2), 2) +
        cos(radians(latitude)) * cos(radians(centroid_lat)) *
        pow(sin(radians(centroid_lon - longitude) / 2), 2)
    )) as distance_from_centroid_km
from {{ ref('station_clusters') }}  -- dbt seed
order by cluster_id, distance_from_centroid_km
```

**Columns:**
- `station_id`, `station_name`, `latitude`, `longitude` (from dim_stations)
- `cluster_id` (0-29) - K-means cluster assignment
- `centroid_lat`, `centroid_lon` - Cluster center coordinates
- `distance_from_centroid_km` - How far station is from cluster center (for sorting)

**Materialization:** `table`

**Usage:**
```bash
# Generate clusters (run once, or when station data changes)
uv run python scripts/generate_station_clusters.py

# Load into dbt
cd dbt
uv run dbt seed  # Loads station_clusters.csv
uv run dbt run --select dim_station_clusters
```

---

#### New Model: `mart_baseline_net_flow` (Marts Layer)

**File:** `dbt/models/marts/mart_baseline_net_flow.sql`

**Purpose:** Pre-aggregate historical hourly net flow patterns by all input dimensions (area, day-of-week, weather, holiday, hour)

**Grain:** One row per unique combination of: `cluster_id + day_of_week + weather_category + holiday_type + hour`

**Weather Categorization Logic:**

```sql
CASE
    WHEN w.precip > 5 THEN 'rainy'        -- Rain > 5mm
    WHEN w.tmax < 10 THEN 'cold'          -- Max temp < 10¬∞C
    WHEN w.tmax > 25 THEN 'hot'           -- Max temp > 25¬∞C
    ELSE 'clear'                           -- Normal conditions
END as weather_category
```

**Holiday Type Logic:**

```sql
CASE
    WHEN h.is_major = true THEN 'major'
    WHEN h.date IS NOT NULL AND h.is_major = false THEN 'minor'
    ELSE 'none'
END as holiday_type
```

**Full SQL:**

```sql
{{
    config(
        materialized='table'
    )
}}

-- Historical Net Flow Baseline for Demand Forecasting
-- Pre-aggregates hourly patterns by cluster, day-of-week, weather, holiday
-- Powers scenario-based simulation in forecast dashboard

with trips_with_context as (
    select
        t.ride_id,
        t.ride_date,
        extract(hour from t.started_at) as hour,
        dayofweek(t.ride_date) as day_of_week,
        t.start_station_id,
        t.end_station_id,
        sc_start.cluster_id as start_cluster_id,
        sc_end.cluster_id as end_cluster_id,
        w.tmax,
        w.precip,
        h.is_major,
        h.date as holiday_date
    from {{ ref('stg_bike_trips') }} t
    left join {{ ref('dim_station_clusters') }} sc_start
        on t.start_station_id = sc_start.station_id
    left join {{ ref('dim_station_clusters') }} sc_end
        on t.end_station_id = sc_end.station_id
    left join {{ ref('stg_weather') }} w
        on t.ride_date = w.date
    left join {{ ref('stg_holidays') }} h
        on t.ride_date = h.date
),

trips_categorized as (
    select
        ride_id,
        ride_date,
        hour,
        day_of_week,
        start_cluster_id,
        end_cluster_id,
        -- Weather categorization
        case
            when precip > 5 then 'rainy'
            when tmax < 10 then 'cold'
            when tmax > 25 then 'hot'
            else 'clear'
        end as weather_category,
        -- Holiday type
        case
            when is_major = true then 'major'
            when holiday_date is not null and is_major = false then 'minor'
            else 'none'
        end as holiday_type
    from trips_with_context
),

-- Calculate net flow: trips_ended - trips_started per cluster per hour
hourly_trips as (
    -- Trips started (bikes leaving cluster)
    select
        start_cluster_id as cluster_id,
        ride_date,
        hour,
        day_of_week,
        weather_category,
        holiday_type,
        1 as trips_started,
        0 as trips_ended
    from trips_categorized
    where start_cluster_id is not null

    union all

    -- Trips ended (bikes arriving at cluster)
    select
        end_cluster_id as cluster_id,
        ride_date,
        hour,
        day_of_week,
        weather_category,
        holiday_type,
        0 as trips_started,
        1 as trips_ended
    from trips_categorized
    where end_cluster_id is not null
),

net_flow_by_day_hour as (
    select
        cluster_id,
        ride_date,
        day_of_week,
        weather_category,
        holiday_type,
        hour,
        sum(trips_ended) - sum(trips_started) as net_flow
    from hourly_trips
    group by cluster_id, ride_date, day_of_week, weather_category, holiday_type, hour
)

-- Final aggregation: average net flow across all historical observations
select
    cluster_id,
    day_of_week,
    weather_category,
    holiday_type,
    hour,
    round(avg(net_flow), 2) as avg_net_flow,
    round(stddev(net_flow), 2) as stddev_net_flow,
    count(*) as sample_size,
    min(net_flow) as min_net_flow,
    max(net_flow) as max_net_flow
from net_flow_by_day_hour
group by cluster_id, day_of_week, weather_category, holiday_type, hour
order by cluster_id, day_of_week, hour
```

**Columns:**
- `cluster_id` (INTEGER, 0-29)
- `day_of_week` (INTEGER, 0=Sunday, 1=Monday, ..., 6=Saturday)
- `weather_category` (VARCHAR: 'clear', 'rainy', 'cold', 'hot')
- `holiday_type` (VARCHAR: 'none', 'major', 'minor')
- `hour` (INTEGER, 0-23)
- `avg_net_flow` (DOUBLE) - Average net bikes to add/remove (negative = add, positive = remove)
- `stddev_net_flow` (DOUBLE) - Standard deviation for confidence intervals
- `sample_size` (INTEGER) - Number of historical data points
- `min_net_flow`, `max_net_flow` (INTEGER) - Range of observed values

**Data Sparsity:** Some combinations may have 0 historical observations (e.g., "hot weather on major holiday at 3am"). Dashboard will show "No data available" message for these scenarios.

**Materialization:** `table`

---

### Component Breakdown

#### Streamlit Dashboard: `streamlit_app/pages/Demand_Forecast.py`

**Page Configuration:**

```python
st.set_page_config(
    page_title="Demand Forecast Simulator",
    page_icon="üîÆ",
    layout="wide"
)
```

**Layout:** Two-column design
- **Left column:** User inputs + cluster selection map
- **Right column:** 24-hour forecast chart + station list

---

**Section 1: User Input Controls (Left Sidebar)**

```python
st.sidebar.header("Scenario Configuration")

# Day of week selector
day_of_week = st.sidebar.selectbox(
    "Day of Week",
    options=[0, 1, 2, 3, 4, 5, 6],
    format_func=lambda x: ['Sunday', 'Monday', 'Tuesday', 'Wednesday',
                           'Thursday', 'Friday', 'Saturday'][x],
    index=1  # Default to Monday
)

# Weather condition selector
weather = st.sidebar.selectbox(
    "Weather Conditions",
    options=['clear', 'rainy', 'cold', 'hot'],
    format_func=lambda x: {
        'clear': '‚òÄÔ∏è Clear (Normal conditions)',
        'rainy': 'üåßÔ∏è Rainy (>5mm precipitation)',
        'cold': '‚ùÑÔ∏è Cold (<10¬∞C)',
        'hot': 'üî• Hot (>25¬∞C)'
    }[x]
)

# Holiday type selector
holiday_type = st.sidebar.selectbox(
    "Holiday Type",
    options=['none', 'major', 'minor'],
    format_func=lambda x: {
        'none': 'Regular Day',
        'major': 'Major Holiday (Memorial Day, July 4th)',
        'minor': 'Minor Holiday'
    }[x]
)
```

---

**Section 2: Interactive Cluster Map (Left Main Area)**

```python
st.header("Select Area on Map")

# Load cluster centroids
@st.cache_data
def load_clusters():
    con = get_db_connection()
    return con.execute("""
        SELECT DISTINCT
            cluster_id,
            centroid_lat,
            centroid_lon,
            COUNT(*) as station_count
        FROM main_core.dim_station_clusters
        GROUP BY cluster_id, centroid_lat, centroid_lon
    """).df()

clusters = load_clusters()

# Create map with cluster markers
import plotly.graph_objects as go

fig = go.Figure(go.Scattermapbox(
    lat=clusters['centroid_lat'],
    lon=clusters['centroid_lon'],
    mode='markers',
    marker=dict(
        size=15,
        color=clusters['cluster_id'],
        colorscale='Viridis',
        showscale=True,
        colorbar=dict(title="Cluster ID")
    ),
    text=clusters['cluster_id'],
    customdata=clusters[['cluster_id', 'station_count']],
    hovertemplate='<b>Cluster %{text}</b><br>%{customdata[1]} stations<extra></extra>',
))

fig.update_layout(
    mapbox=dict(
        style='open-street-map',
        center=dict(lat=40.73, lon=-73.98),  # NYC center
        zoom=11
    ),
    height=500,
    margin=dict(l=0, r=0, t=0, b=0)
)

st.plotly_chart(fig, use_container_width=True)

# Cluster selection dropdown (primary method for MVP)
selected_cluster = st.selectbox(
    "Select Cluster:",
    options=clusters['cluster_id'].tolist(),
    index=0
)
```

**Note:** Map click interaction is not implemented in MVP (Streamlit limitation). Map serves as visual reference; users select via dropdown.

---

**Section 3: 24-Hour Forecast Chart (Right Top)**

```python
st.header(f"üìä 24-Hour Net Flow Forecast")

# Build scenario description
day_names = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']
weather_labels = {'clear': 'Clear', 'rainy': 'Rainy', 'cold': 'Cold', 'hot': 'Hot'}
holiday_labels = {'none': 'Regular Day', 'major': 'Major Holiday', 'minor': 'Minor Holiday'}

st.markdown(f"**Scenario:** {day_names[day_of_week]}, {weather_labels[weather]}, {holiday_labels[holiday_type]}, Cluster {selected_cluster}")

# Load forecast data
@st.cache_data
def load_forecast(cluster_id, day, weather_cat, holiday):
    con = get_db_connection()
    return con.execute("""
        SELECT
            hour,
            avg_net_flow,
            stddev_net_flow,
            sample_size
        FROM main_marts.mart_baseline_net_flow
        WHERE cluster_id = ?
          AND day_of_week = ?
          AND weather_category = ?
          AND holiday_type = ?
        ORDER BY hour
    """, [cluster_id, day, weather_cat, holiday]).df()

forecast = load_forecast(selected_cluster, day_of_week, weather, holiday_type)

if forecast.empty:
    st.warning("‚ö†Ô∏è No historical data for this scenario combination. Try adjusting inputs.")
else:
    # Data quality warning
    if forecast['sample_size'].min() < 3:
        st.info("‚ÑπÔ∏è Limited data for some hours. Predictions may be less reliable.")

    # Create bar chart (positive = add bikes, negative = remove bikes)
    import plotly.graph_objects as go

    fig = go.Figure(go.Bar(
        x=forecast['hour'],
        y=forecast['avg_net_flow'],
        marker_color=forecast['avg_net_flow'].apply(
            lambda x: '#FF6B6B' if x < 0 else '#4ECDC4'  # Red=remove, Teal=add
        ),
        text=forecast['avg_net_flow'].round(1),
        textposition='outside',
        hovertemplate='Hour %{x}:00<br>Net Flow: %{y:.1f} bikes<br>Sample Size: %{customdata}<extra></extra>',
        customdata=forecast['sample_size']
    ))

    fig.update_layout(
        title="Bikes to Add (+) or Remove (-) by Hour",
        xaxis_title="Hour of Day",
        yaxis_title="Net Flow (bikes)",
        xaxis=dict(tickmode='linear', tick0=0, dtick=1),
        height=400,
        hovermode='x unified'
    )

    # Add zero reference line
    fig.add_hline(y=0, line_dash="dash", line_color="gray", opacity=0.5)

    st.plotly_chart(fig, use_container_width=True)

    # Summary metrics
    col1, col2, col3 = st.columns(3)
    with col1:
        peak_add = forecast['avg_net_flow'].max()
        st.metric("Peak Addition", f"+{peak_add:.0f} bikes" if peak_add > 0 else "None")
    with col2:
        peak_remove = forecast['avg_net_flow'].min()
        st.metric("Peak Removal", f"{peak_remove:.0f} bikes" if peak_remove < 0 else "None")
    with col3:
        avg_sample = forecast['sample_size'].mean()
        st.metric("Avg Data Points/Hour", f"{avg_sample:.0f}")
```

---

**Section 4: Station List in Selected Cluster (Right Bottom)**

```python
st.header(f"üö≤ Stations in Cluster {selected_cluster}")

# Load stations
@st.cache_data
def load_cluster_stations(cluster_id):
    con = get_db_connection()
    return con.execute("""
        SELECT
            station_id,
            station_name,
            latitude,
            longitude,
            round(distance_from_centroid_km, 3) as distance_km
        FROM main_core.dim_station_clusters
        WHERE cluster_id = ?
        ORDER BY distance_from_centroid_km
    """, [cluster_id]).df()

stations = load_cluster_stations(selected_cluster)

st.dataframe(
    stations,
    column_config={
        "station_id": "ID",
        "station_name": "Station Name",
        "latitude": st.column_config.NumberColumn("Lat", format="%.4f"),
        "longitude": st.column_config.NumberColumn("Lon", format="%.4f"),
        "distance_km": st.column_config.NumberColumn("Distance from Center (km)", format="%.3f")
    },
    hide_index=True,
    use_container_width=True,
    height=400
)

st.caption(f"**{len(stations)} stations** in this cluster")
```

---

### Logic / Algorithm

#### Net Flow Calculation

Net flow is calculated at the hourly granularity per cluster:

```
net_flow = trips_ended_at_cluster - trips_started_from_cluster

If net_flow < 0: Bikes depleting ‚Üí Add bikes (e.g., morning commute from residential)
If net_flow > 0: Bikes accumulating ‚Üí Remove bikes (e.g., evening return to residential)
```

**Example:**
- Cluster 5 (Financial District), Monday 8am, Clear weather, Regular day
- Historical average: 120 trips started, 30 trips ended
- Net flow = 30 - 120 = **-90 bikes** ‚Üí **Add 90 bikes before 8am**

---

#### Scenario Lookup (Dashboard Logic)

The forecasting is a simple filtered query (no ML, no complex calculations):

```python
def get_forecast(cluster_id, day_of_week, weather, holiday_type):
    """
    Retrieve historical baseline for given scenario.

    Returns: DataFrame with columns [hour, avg_net_flow, stddev_net_flow, sample_size]
    """
    query = """
        SELECT hour, avg_net_flow, stddev_net_flow, sample_size
        FROM main_marts.mart_baseline_net_flow
        WHERE cluster_id = ?
          AND day_of_week = ?
          AND weather_category = ?
          AND holiday_type = ?
        ORDER BY hour
    """
    return conn.execute(query, [cluster_id, day_of_week, weather, holiday_type]).df()
```

**Handling Missing Data:**

If query returns 0 rows (no historical observations for that scenario):
- Display warning: "No historical data for this combination"
- Suggest: "Try selecting a different weather condition or cluster"

**Future Enhancement:** Fall back to broader category (e.g., if "rainy + major holiday + Monday 3am" has no data, fall back to "rainy + regular day + Monday 3am")

---

## 3. Impact and Risk Analysis

### System Dependencies

**Depends On (Existing):**
- `stg_bike_trips` - Bike trip data with station IDs and timestamps (no changes needed)
- `stg_weather` - Weather data with temperature and precipitation (no changes needed)
- `stg_holidays` - Holiday data with is_major flag (no changes needed)
- `dim_stations` - Station master data with lat/lon (no changes needed)

**Affects:**
- None (all new models, no modifications to existing)

**Consumed By:**
- New Streamlit page `Demand_Forecast.py`

**No Impact On:**
- Existing dlt pipelines (`bike.py`, `weather.py`, `holidays.py`, `games.py`)
- Existing marts (`mart_demand_daily`, `mart_weather_effect`, `mart_holiday_impact_*`, `mart_game_*`)
- Existing Streamlit pages (`Home.py`, `Weather.py`, `Holiday_Impact.py`, `Game_Impact.py`)
- Airflow DAG (forecasting is on-demand, not scheduled)

---

### Potential Risks & Mitigations

#### Risk 1: Data Sparsity for Rare Scenarios

**Description:** Some combinations (e.g., hot weather + major holiday + 3am) may have 0 or very few historical observations in May-June 2024 data (only 61 days).

**Likelihood:** High - Limited temporal coverage.

**Impact:** Medium - Dashboard shows "No data" for some user selections, reducing usability.

**Mitigation:**
- Display `sample_size` in chart tooltip so users see data quality
- Show warning when `sample_size < 3`
- Document limitation in dashboard caption: "Based on May-June 2024 data only"
- Future: Implement fallback logic (broaden filters if exact match empty)

**Code:**
```python
if forecast.empty:
    st.warning("‚ö†Ô∏è No historical data for this scenario. Try different inputs.")
elif forecast['sample_size'].min() < 3:
    st.info("‚ÑπÔ∏è Limited data for some hours. Predictions may be less reliable.")
```

---

#### Risk 2: K-Means Cluster Stability

**Description:** Re-running k-means with different random seeds may produce different cluster assignments, breaking consistency.

**Likelihood:** Medium - K-means is sensitive to initialization.

**Impact:** Low - Only affects consistency if clusters regenerated frequently.

**Mitigation:**
- Use fixed `random_state=42` in scikit-learn for reproducibility
- Document that clusters should NOT be regenerated frequently (only when station network changes)
- Store clusters as dbt seed (version controlled in git)
- Add comment in script warning about regeneration impact

**Code:**
```python
# IMPORTANT: Use fixed random_state for reproducibility
# Only regenerate clusters if station network changes significantly
kmeans = KMeans(n_clusters=30, random_state=42, n_init=10)
```

---

#### Risk 3: Map Click Interaction Complexity

**Description:** Plotly click events in Streamlit require workarounds (not natively supported without additional libraries).

**Likelihood:** Certain - Known Streamlit limitation.

**Impact:** Low - Fallback to dropdown works fine for MVP.

**Mitigation:**
- Use dropdown selector as primary method (MVP)
- Map serves as visual reference only (shows all clusters, helps user understand geography)
- Future: Use `streamlit-plotly-events` library for true click interaction
- Document in code that map is non-interactive in current version

**Code (MVP approach):**
```python
# Map shows clusters visually (not clickable in MVP)
st.plotly_chart(fig, use_container_width=True)

# Dropdown is primary selection method
selected_cluster = st.selectbox("Select Cluster", clusters['cluster_id'])
```

---

#### Risk 4: Weather Categorization Simplicity

**Description:** 4 weather categories ('clear', 'rainy', 'cold', 'hot') oversimplify conditions. Real weather is multi-dimensional (temp + rain + wind simultaneously).

**Likelihood:** Certain - This is a known simplification.

**Impact:** Medium - Some scenarios conflated (e.g., "rainy and cold" grouped as "rainy only").

**Mitigation:**
- Start with simple categories for MVP (easier to understand and explain)
- Prioritize rain categorization since it has biggest impact on bike demand
- Document categorization logic in code comments and dashboard help text
- Future: Add multi-select weather conditions or finer categories
- Note: May-June 2024 data has limited temperature variance, so cold/hot may have sparse data

**Rationale:** Rain is the dominant weather factor. Cold/hot categories may have insufficient historical observations in spring/summer dataset.

---

#### Risk 5: Large mart_baseline_net_flow Table Size

**Description:** Cartesian product of dimensions creates large table:
- 30 clusters √ó 7 days √ó 4 weather √ó 3 holiday types √ó 24 hours = **60,480 potential rows**

**Likelihood:** Low - Most combinations will be empty (no historical data).

**Impact:** Low - DuckDB handles tens of thousands of rows easily.

**Mitigation:**
- Only store rows with `sample_size > 0` (actual observations exist)
- DuckDB's columnar storage compresses efficiently
- Monitor table size: `SELECT COUNT(*) FROM mart_baseline_net_flow`
- Expected actual size: 5,000-15,000 rows (many sparse combinations filtered out)
- Add comment in SQL explaining why many combinations are missing

**Performance:** Even with 60K rows, DuckDB query performance will be <100ms due to indexed filtering.

---

## 4. Testing Strategy

### dbt Model Tests

Add to `dbt/models/core/schema.yml`:

```yaml
models:
  - name: dim_station_clusters
    description: "K-means cluster assignments for bike stations (k=30)"
    columns:
      - name: station_id
        description: "Unique station identifier"
        tests:
          - unique
          - not_null
      - name: cluster_id
        description: "Cluster assignment (0-29)"
        tests:
          - not_null
          - dbt_utils.accepted_range:
              min_value: 0
              max_value: 29
      - name: distance_from_centroid_km
        description: "Distance from station to cluster center"
        tests:
          - not_null
          - dbt_utils.accepted_range:
              min_value: 0
              max_value: 5  # Clusters shouldn't span >5km radius
```

Add to `dbt/models/marts/schema.yml`:

```yaml
models:
  - name: mart_baseline_net_flow
    description: "Historical hourly net flow patterns by area/day/weather/holiday for demand forecasting"
    columns:
      - name: cluster_id
        description: "Cluster identifier"
        tests:
          - not_null
          - dbt_utils.accepted_range:
              min_value: 0
              max_value: 29
      - name: day_of_week
        description: "Day of week (0=Sunday, 6=Saturday)"
        tests:
          - not_null
          - dbt_utils.accepted_range:
              min_value: 0
              max_value: 6
      - name: weather_category
        description: "Weather condition category"
        tests:
          - not_null
          - accepted_values:
              values: ['clear', 'rainy', 'cold', 'hot']
      - name: holiday_type
        description: "Holiday classification"
        tests:
          - not_null
          - accepted_values:
              values: ['none', 'major', 'minor']
      - name: hour
        description: "Hour of day (0-23)"
        tests:
          - not_null
          - dbt_utils.accepted_range:
              min_value: 0
              max_value: 23
      - name: sample_size
        description: "Number of historical observations"
        tests:
          - not_null
          - dbt_utils.expression_is_true:
              expression: "> 0"
```

---

### Integration Testing

**End-to-End Test Plan:**

**1. Generate Station Clusters:**

```bash
# Run cluster generation script
uv run python scripts/generate_station_clusters.py

# Expected output:
# Generated 2000+ station assignments across 30 clusters
# File created: dbt/seeds/station_clusters.csv

# Verify CSV file
head dbt/seeds/station_clusters.csv
# Expected columns: station_id, station_name, latitude, longitude, cluster_id, centroid_lat, centroid_lon
```

**2. Build dbt Models:**

```bash
cd dbt

# Load seed file
uv run dbt seed

# Build cluster and baseline models
uv run dbt run --select +dim_station_clusters +mart_baseline_net_flow

# Run tests
uv run dbt test --select +dim_station_clusters +mart_baseline_net_flow

# Expected: All tests pass (column constraints, value ranges)
```

**3. Data Quality Verification:**

```sql
-- Check cluster distribution
SELECT cluster_id, COUNT(*) as station_count
FROM main_core.dim_station_clusters
GROUP BY cluster_id
ORDER BY cluster_id;
-- Expected: Roughly balanced distribution (50-100 stations per cluster)
-- Expected: All cluster_ids from 0-29 present

-- Check cluster geographic spread
SELECT
    cluster_id,
    MIN(distance_from_centroid_km) as min_dist,
    MAX(distance_from_centroid_km) as max_dist,
    AVG(distance_from_centroid_km) as avg_dist
FROM main_core.dim_station_clusters
GROUP BY cluster_id
ORDER BY avg_dist DESC;
-- Expected: Max distance < 5km per cluster
-- Expected: Avg distance ~0.5-1.5km

-- Check baseline mart coverage
SELECT
    weather_category,
    holiday_type,
    COUNT(DISTINCT cluster_id) as clusters_with_data,
    COUNT(*) as total_scenarios,
    SUM(sample_size) as total_observations
FROM main_marts.mart_baseline_net_flow
GROUP BY weather_category, holiday_type
ORDER BY total_observations DESC;
-- Expected: 'clear' + 'none' has most data (~20,000+ observations)
-- Expected: 'hot' + 'major' has least data (may be 0)

-- Validate net flow calculation for typical scenario
SELECT
    cluster_id,
    hour,
    avg_net_flow,
    sample_size
FROM main_marts.mart_baseline_net_flow
WHERE day_of_week = 1  -- Monday
  AND weather_category = 'clear'
  AND holiday_type = 'none'
  AND cluster_id = 0
ORDER BY hour;
-- Expected: 24 rows (hours 0-23)
-- Expected: Morning rush hours (7-9am) show negative net flow (bikes depleting)
-- Expected: Evening rush hours (5-7pm) show positive net flow (bikes returning)
-- Expected: Sample_size > 5 for most hours
```

**4. Dashboard Testing:**

```bash
cd ..
uv run streamlit run streamlit_app/pages/Demand_Forecast.py
```

**Manual Dashboard Tests:**

1. **Default scenario test:**
   - Select: Monday, Clear weather, Regular day, Cluster 0
   - Expected: Chart shows typical weekday commute pattern
   - Expected: Morning negative net flow (add bikes), evening positive (remove bikes)
   - Expected: Station list shows 50-100 stations sorted by distance

2. **Financial District pattern test:**
   - Find cluster in lower Manhattan (financial area)
   - Select: Monday, Clear, Regular day
   - Expected: Large negative net flow in morning (bikes leaving residential ‚Üí offices)
   - Expected: Large positive net flow in evening (bikes returning)

3. **Rainy day test:**
   - Select: Any day, Rainy weather, Regular day, Any cluster
   - Expected: Overall lower absolute net flow (fewer trips)
   - Expected: Some scenarios may show "No data" warning

4. **Holiday test:**
   - Select: Monday, Clear, Major holiday, Any cluster
   - Expected: Different pattern than regular Monday (reduced commute peaks)
   - Expected: Some hours may have limited data warning

5. **Data sparsity test:**
   - Select: Hot weather + Major holiday (rare combination)
   - Expected: "No historical data" warning appears
   - Expected: Chart does not render (empty state)

6. **Map visualization test:**
   - Check that map displays 30 cluster markers
   - Verify clusters centered over NYC geography
   - Verify hover shows cluster ID and station count

7. **Responsive design test:**
   - Resize browser window to mobile width
   - Expected: Layout adjusts, all controls accessible
   - Expected: Chart and table remain readable

---

### Unit Testing (Optional)

**File:** `tests/test_forecast_engine.py`

```python
def test_weather_categorization():
    """Test weather category logic matches SQL."""
    def categorize_weather(tmax, precip):
        if precip > 5:
            return 'rainy'
        elif tmax < 10:
            return 'cold'
        elif tmax > 25:
            return 'hot'
        else:
            return 'clear'

    assert categorize_weather(tmax=20, precip=0) == 'clear'
    assert categorize_weather(tmax=20, precip=10) == 'rainy'
    assert categorize_weather(tmax=5, precip=0) == 'cold'
    assert categorize_weather(tmax=30, precip=0) == 'hot'

def test_net_flow_calculation():
    """Verify net flow = trips_ended - trips_started."""
    trips_ended = 50
    trips_started = 120
    net_flow = trips_ended - trips_started

    assert net_flow == -70
    assert net_flow < 0  # Bikes depleting ‚Üí should add bikes

def test_net_flow_interpretation():
    """Test operational interpretation of net flow."""
    def get_recommendation(net_flow):
        if net_flow < 0:
            return f"Add {abs(net_flow)} bikes"
        elif net_flow > 0:
            return f"Remove {net_flow} bikes"
        else:
            return "No action needed"

    assert get_recommendation(-50) == "Add 50 bikes"
    assert get_recommendation(30) == "Remove 30 bikes"
    assert get_recommendation(0) == "No action needed"
```

---

### Performance Benchmarks

**Targets:**
- Cluster generation script: < 30 seconds
- dbt build (both models): < 3 minutes
- Streamlit first load (with caching): < 3 seconds
- Streamlit forecast query: < 500ms
- Map rendering: < 2 seconds

**Measurement Commands:**

```bash
# Cluster generation
time uv run python scripts/generate_station_clusters.py
# Target: <30 seconds

# dbt build
cd dbt
time uv run dbt run --select +mart_baseline_net_flow
# Target: <3 minutes

# Query performance (in DuckDB CLI)
.timer on
SELECT * FROM main_marts.mart_baseline_net_flow
WHERE cluster_id = 5 AND day_of_week = 1 AND weather_category = 'clear' AND holiday_type = 'none';
# Target: <100ms
```

---

**End of Technical Specification**
